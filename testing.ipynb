{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0')"
  },
  "interpreter": {
   "hash": "343b6ac7eff0d382f058e14c2ddeb6cb57577d740f11beca8e43569b1defe6e1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Testing notebook playground for extracting data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector as msc\n",
    "import json\n",
    "import tweepy\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import pprint\n",
    "import datetime\n",
    "from dateutil.rrule import rrule, DAILY\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config file into JSON object\n",
    "configFile = open('config.json')\n",
    "config = json.load(configFile)\n",
    "\n",
    "# make connection to the GCloud database\n",
    "db_config = config['database']\n",
    "connection = msc.connect(\n",
    "    host=db_config['host'], \n",
    "    port=db_config['port'],\n",
    "    user=db_config['user'],\n",
    "    password=db_config['password'],\n",
    "    database=db_config['database']\n",
    ")\n",
    "\n",
    "# authorize Twitter app with Tweepy OAuthHandler\n",
    "twit_config = config['twitter']\n",
    "auth = tweepy.OAuthHandler(twit_config['api_key'], twit_config['api_key_secret'])\n",
    "auth.set_access_token(twit_config['access_token'], twit_config['access_token_secret'])\n",
    "twitapi = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract Tweets from Malaysian MOH about Malaysian COVID-19 case data\n",
    "# write to pandas database\n",
    "tweet_collection = twitapi.search(\"KKMPutrajaya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user id of @KKMPutrajaya\n",
    "url = \"https://api.twitter.com\"\n",
    "headers = {\"Authorization\": \"Bearer {}\".format(twit_config['bearer_token'])}\n",
    "response = requests.request(\"GET\", url + \"/2/users/by/username/KKMPutrajaya\", headers=headers)\n",
    "user_id = response.json()['data']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize pandas data set\n",
    "tweets = []\n",
    "next_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get posts from Malaysian MOH (@KKMPutrajaya)\n",
    "search_url = (url + \"/2/users/{}/tweets\".format(user_id))\n",
    "response = requests.request(\"GET\", search_url, headers=headers)\n",
    "data = response.json()['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch tweet data\n",
    "# this fetches thousands of MOH's tweets, which are factored into the quota for my \n",
    "# Twitter account, so beware!\n",
    "def fetch_data():\n",
    "\n",
    "    # get posts from Malaysian MOH (@KKMPutrajaya)\n",
    "    search_url = (url + \"/2/users/{}/tweets\".format(user_id))\n",
    "    response = requests.request(\"GET\", search_url, headers=headers)\n",
    "    data = response.json()['data']\n",
    "\n",
    "    # cycle and add to data\n",
    "    for tweet in data:\n",
    "        if \"Status Terkini #COVID19\" in tweet['text']:\n",
    "            tweets.append(tweet)\n",
    "    # get next pagination token\n",
    "    next_token = response.json()['meta'].get('next_token')\n",
    "    next_tokens.append(next_token)\n",
    "    # while next token is not None (ie there is still a next page)\n",
    "    while next_token is not None:\n",
    "        # get next page\n",
    "        search_url = (url + \"/2/users/{}/tweets?pagination_token={}\".format(user_id, next_token))\n",
    "        response = requests.request(\"GET\", search_url, headers=headers)\n",
    "        data = response.json()['data']\n",
    "        next_token = response.json()['meta'].get('next_token')\n",
    "        # cycle through the next page\n",
    "        for tweet in data:\n",
    "            if \"Status Terkini #COVID19\" in tweet['text']:\n",
    "                tweets.append(tweet)\n",
    "                print(tweet)\n",
    "                next_tokens.append(next_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsdf = pd.DataFrame(tweets)\n",
    "tweetsdf = tweetsdf[0:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_tokensdf = pd.DataFrame(next_tokens)\n",
    "next_tokensdf = next_tokensdf[0:102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      Status Terkini #COVID19, 22 Jun 2021 \\nKes sem...\n",
       "1      Status Terkini #COVID19, 21 Jun 2021 \\nKes sem...\n",
       "2      Status Terkini #COVID19, 20 Jun 2021 \\nKes sem...\n",
       "3      Status Terkini #COVID19, 19 Jun 2021 \\nKes sem...\n",
       "4      Status Terkini #COVID19, 18 Jun 2021 \\nKes sem...\n",
       "                             ...                        \n",
       "96     Status Terkini #COVID19, 17 Mac 2021 \\n\\nKes s...\n",
       "97     Status Terkini #COVID19, 16 Mac 2021 \\n\\nKes s...\n",
       "98     Status Terkini #COVID19, 15 Mac 2021 \\n\\nKes s...\n",
       "99     Status Terkini #COVID19, 14 Mac 2021 \\n\\nKes s...\n",
       "100    Status Terkini #COVID19, 13 Mac 2021 \\n\\nKes s...\n",
       "Name: text, Length: 101, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "tweetsdf['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Status Terkini #COVID19, 22 Jun 2021 \\nKes sembuh= 5,557\\nJumlah kes sembuh= 639,181\\nKes baharu positif= 4,743 (2 import) \\nJumlah positif= 705,762\\nKes kematian= 77\\nJumlah kes kematian= 4,554\\nKes dirawat di ICU= 875\\nBantuan alat pernafasan= 445 https://t.co/MZHf51NU0C'"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "tweetsdf['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Status Terkini #COVID19, 13 Mac 2021 \\n\\nKes sembuh=1,830\\nJumlah kes sembuh=304,492 kes \\nKes baharu positif=1,470 kes (1,458 tempatan, 12 import) \\nJumlah positif=322,409\\nKes kematian=3\\nJumlah kes kematian=1,206 kes \\nKes dirawat di ICU=162 kes \\nBantuan Alat Pernafasan=70 kes https://t.co/S6kwwkmLZw'"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "tweetsdf['text'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'meta': {'result_count': 0,\n",
       "  'previous_token': '77qpymm88g5h9vqklulfea6i0etntp4c28y39657vvi9l'}}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "search_url = (url + \"/2/users/{}/tweets?pagination_token={}\".format(user_id, \"7140dibdnow9c7btw3w3xuo59eonz5ff1wirbd9w1h7ao\"))\n",
    "response = requests.request(\"GET\", search_url, headers=headers)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'next_token' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-924d7e0bcd08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# function to cycle through pages automatically\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgoForward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mnext_token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnext_token\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'next_token' is not defined"
     ]
    }
   ],
   "source": [
    "# function to cycle through pages automatically\n",
    "print(next_token)\n",
    "def goForward():\n",
    "    global next_token\n",
    "    if next_token is None:\n",
    "        raise Exception(\"no next page\")\n",
    "    else:\n",
    "        search_url = (url + \"/2/users/{}/tweets?end_time={}&pagination_token={}\".format(user_id, \"2021-03-11T12:00:00Z\", next_token))\n",
    "        response = requests.request(\"GET\", search_url, headers=headers)\n",
    "        next_token = response.json()['meta'].get('next_token')\n",
    "        print(response.json()['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweet = tweetsdf.iloc[0, :]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert python date to DD-MMM-YYYY, where \"MMM\" is the month in malay\n",
    "month_dict = [\n",
    "    'januari',\n",
    "    'februari',\n",
    "    'mac',\n",
    "    'april',\n",
    "    'mei',\n",
    "    'jun',\n",
    "    'julai',\n",
    "    'ogos',\n",
    "    'september',\n",
    "    'oktober',\n",
    "    'november',\n",
    "    'disember'\n",
    "]\n",
    "\n",
    "# month is in Malay (like the above)\n",
    "def parseDate(day, month, year):\n",
    "    return datetime.date(int(year), int(list(map(lambda m: m.title(), month_dict)).index(month))+1, int(day))\n",
    "\n",
    "def convertDate(date):\n",
    "    return date.strftime(\"%d-{}-%Y\".format(month_dict[int(date.strftime(\"%m\"))-1]).title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "datetime.date(2021, 4, 15)"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "parseDate(\"15\", \"April\", \"2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser for tweet content\n",
    "def parseTweetContent(tweet):\n",
    "    # print(tweet)\n",
    "    # get tweet content without \\n and \n",
    "    # explode it via spaces\n",
    "    exploded_tweet = [x for x in re.split('\\n| |=', tweet.replace('\\n\\n', '\\n')) if x]\n",
    "    # print(exploded_tweet)\n",
    "    \n",
    "    # get date as \"YYYY MMM DD\", where the month is in Malay\n",
    "    date = parseDate(exploded_tweet[3], exploded_tweet[4], exploded_tweet[5])\n",
    "    # print(date)\n",
    "\n",
    "    try:\n",
    "\n",
    "        # get other data\n",
    "        # the numbers represent the indexes at which the relevant values are in\n",
    "        # the exploded tweet\n",
    "        data_dict = {\n",
    "            \"cured_cases\": 8,\n",
    "            \"total_cured_cases\": 12,\n",
    "            \"new_cases\": 16,\n",
    "            \"total_new_cases\": 23 if date <= datetime.date(2021, 6, 9) else 21,\n",
    "            \"deaths\": 26 if date <= datetime.date(2021, 6, 9) else 24,\n",
    "            \"total_deaths\": 30 if date <= datetime.date(2021, 6, 9) else 28,\n",
    "            \"icu_cases\": 35 if date <= datetime.date(2021, 6, 9) else 33,\n",
    "            \"resp_cases\": 39 if date <= datetime.date(2021, 6, 9) else 37\n",
    "        }\n",
    "        \n",
    "        # parse data\n",
    "        result = {name: int(exploded_tweet[value].replace(',', '')) for name, value in data_dict.items()} \n",
    "\n",
    "        # add date\n",
    "        result['date'] = date.strftime('%d %m %Y')\n",
    "        # return result\n",
    "        return result\n",
    "    except BaseException as e:\n",
    "        print(e)\n",
    "        print('cannot parse tweet at date {}'.format(date))\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "invalid literal for int() with base 10: 'positif'\ncannot parse tweet at date 2021-03-13\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "execution_count": 192
    }
   ],
   "source": [
    "# example tweet content\n",
    "parseTweetContent(tweetsdf['text'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "invalid literal for int() with base 10: 'positif'\ncannot parse tweet at date 2021-03-13\n"
     ]
    }
   ],
   "source": [
    "# store data into csv\n",
    "parsed_data = list(map(parseTweetContent, list(tweetsdf['text'])))\n",
    "# parsed_data += ({\n",
    "#     \"cured_cases\": 1830,\n",
    "#     \"total_cured_cases\": 304492, \n",
    "#     \"new_cases\": 1470,\n",
    "#     \"total_new_cases\": 322409, \n",
    "#     \"deaths\": 3,\n",
    "#     \"total_deaths\": 1206, \n",
    "#     \"icu_cases\": 162,\n",
    "#     \"resp_cases\": 70,\n",
    "#     \"date\": \"13 03 2021\"\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(parsed_data)\n",
    "df.to_csv(\"covid_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset\n",
    "dataset = []\n",
    "# initialize variable to text mapping\n",
    "vtt_map = {\n",
    "    'cured_cases': 'Kes sembuh',\n",
    "    'new_cases': 'Kes baharu',\n",
    "    'import_cases': 'Kes import',\n",
    "    'local_cases': 'Kes tempatan',\n",
    "    'active_cases': 'Kes aktif',\n",
    "    'resp_asst_cases': 'Kes yang memerlukan rawatan',\n",
    "    'death_cases': 'Kes kematian',\n",
    "    'number_of_clusters': 'Jumlah kluster',\n",
    "    'number_of_new_clusters': 'Jumlah kluster baharu',\n",
    "    'number_of_expired_clusters': 'Jumlah kluster yang telah tamat',\n",
    "    'number_of_active_clusters': 'Jumlah kluster aktif'\n",
    "}"
   ]
  },
  {
   "source": [
    "### Testing out web scraper for kpkesihatan.com instead to fetch data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['21-januari-2021',\n",
       " '21-februari-2021',\n",
       " '21-mac-2021',\n",
       " '21-april-2021',\n",
       " '21-mei-2021',\n",
       " '21-jun-2021',\n",
       " '21-julai-2021',\n",
       " '21-ogos-2021',\n",
       " '21-september-2021',\n",
       " '21-oktober-2021',\n",
       " '21-november-2021',\n",
       " '21-disember-2021']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "[convertDate(datetime.datetime(2021, i, 21)) for i in range(1, 13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can do formatting on this later\n",
    "def makeURL(date):\n",
    "    return \"https://kpkesihatan.com/{}/kenyataan-akhbar-kpk-{}-situasi-semasa-jangkitan-penyakit-coronavirus-2019-covid-19-di-malaysia/\".format(\n",
    "        date.strftime(\"%d/%m/%Y\"), \n",
    "        convertDate(date)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://kpkesihatan.com/21/06/2021/kenyataan-akhbar-kpk-21-jun-2021-situasi-semasa-jangkitan-penyakit-coronavirus-2019-covid-19-di-malaysia/'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "makeURL(datetime.datetime(2021, 6, 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-cc4f619301db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create soup specifically for the response content\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msoup_cases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Create variable to (Malay) text mapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m vtt_map = {\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m'cured_cases'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Kes sembuh'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Create soup specifically for the response content\n",
    "soup_cases = BeautifulSoup(str(result[1]), 'html.parser')\n",
    "# Create variable to (Malay) text mapping\n",
    "vtt_map = {\n",
    "    'cured_cases': 'Kes sembuh',\n",
    "    'new_cases': 'Kes baharu',\n",
    "    'import_cases': 'Kes import',\n",
    "    'local_cases': 'Kes tempatan',\n",
    "    'active_cases': 'Kes aktif',\n",
    "    'resp_asst_cases': 'Kes yang memerlukan rawatan',\n",
    "    'death_cases': 'Kes kematian',\n",
    "    'number_of_clusters': 'Jumlah kluster',\n",
    "    'number_of_new_clusters': 'Jumlah kluster baharu',\n",
    "    'number_of_expired_clusters': 'Jumlah kluster yang telah tamat',\n",
    "    'number_of_active_clusters': 'Jumlah kluster aktif'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data into dict object\n",
    "day_data = {}\n",
    "for var in vtt_map:\n",
    "    # Get the string containing the key words\n",
    "    keyword_string = str(soup_cases.find(lambda tag: tag.name == \"li\" and vtt_map[var] in tag.text))\n",
    "    # Get the soup for the specific string\n",
    "    soup_var = BeautifulSoup(keyword_string, 'html.parser')\n",
    "    # use the custom soup to extract the case data, which is stored into the day_data object\n",
    "    content = soup_var.find(lambda tag: tag.name == \"strong\").contents[0]\n",
    "    # parse the content to only extract the integer value for the corresponding variable\n",
    "    for s in [\"\\xa0\", \"kes\", \"kluster\", \" \", \",\"]:\n",
    "        content = content.replace(s, \"\")\n",
    "    # convert value to int\n",
    "    day_data[var] = int(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'cured_cases': 5941,\n",
       " 'new_cases': 5293,\n",
       " 'import_cases': 7,\n",
       " 'local_cases': 5286,\n",
       " 'active_cases': 63815,\n",
       " 'resp_asst_cases': 880,\n",
       " 'death_cases': 60,\n",
       " 'number_of_clusters': 2623,\n",
       " 'number_of_new_clusters': 19,\n",
       " 'number_of_expired_clusters': 1789,\n",
       " 'number_of_active_clusters': 834}"
      ]
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "day_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get data from website by date\n",
    "def get_data(date):\n",
    "    # make url to website for date\n",
    "    url = makeURL(date)\n",
    "\n",
    "    # get content of website\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # filter for the case data\n",
    "    # create soup object\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # get all bulleted lists\n",
    "    result = soup.find_all('ul')\n",
    "    # the case data will be the second element\n",
    "    cases_content = str(result[1])\n",
    "\n",
    "    # get soup for cases\n",
    "    soup_cases = BeautifulSoup(cases_content, 'html.parser')\n",
    "\n",
    "    # store the data into the result object\n",
    "    day_data = {}\n",
    "    for var in vtt_map:\n",
    "        # Get the string containing the key words\n",
    "        keyword_string = str(soup_cases.find(lambda tag: tag.name == \"li\" and vtt_map[var] in tag.text))\n",
    "        # Get the soup for the specific string\n",
    "        # soup_var = BeautifulSoup(keyword_string, 'html.parser')\n",
    "        # use the custom soup to extract the case data, which is stored into the day_data object\n",
    "        # content = soup_var.find(lambda tag: tag.name == \"strong\")\n",
    "        # parse the content to only extract the integer value for the corresponding variable\n",
    "        # for s in [\"\\xa0\", \"kes\", \"kluster\", \" \", \",\", \";\"]:\n",
    "        #     content = content.replace(s, \"\")\n",
    "        # convert value to int\n",
    "        day_data[var] = keyword_string\n",
    "\n",
    "    # put date into data\n",
    "    day_data['date'] = date.strftime(\"%d/%m/%Y\")\n",
    "\n",
    "    # return result when done\n",
    "    return day_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'cured_cases': '<li>Kes sembuh : <strong>1,910 kes </strong>(358,726 kes kumulatif, 94.0%);</li>',\n",
       " 'new_cases': '<li>Kes baharu :<strong> 2,340 kes</strong> (381,813 kes kumulatif);</li>',\n",
       " 'import_cases': '<li>Kes import : <strong>12 kes </strong>(1 warganegara, 11 bukan warganegara);<strong></strong></li>',\n",
       " 'local_cases': '<li>Kes tempatan : <strong>2,328 </strong><strong>kes </strong>[2,136 warganegara (91.8%); 192 bukan warganegara (8.2%)];</li>',\n",
       " 'active_cases': '<li>Kes aktif :<strong> 21,687 kes;</strong></li>',\n",
       " 'resp_asst_cases': '<li>Kes yang memerlukan rawatan di Unit Rawatan Rapi (ICU) :<strong> 248 kes</strong>;</li>',\n",
       " 'death_cases': '<li>Kes kematian :<strong> 11 kes </strong>(1,400 kes kumulatif, 0.37%; 9 warganegara; 2 bukan warganegara).</li>',\n",
       " 'number_of_clusters': 'None',\n",
       " 'number_of_new_clusters': 'None',\n",
       " 'number_of_expired_clusters': 'None',\n",
       " 'number_of_active_clusters': 'None',\n",
       " 'date': '21/04/2021'}"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "get_data(datetime.date(2021, 4, 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'cured_cases': '<li>Kes sembuh : <strong>1,346 kes </strong>(310,958 kes kumulatif, 95.0%);</li>',\n",
       " 'new_cases': '<li>Kes baharu : <strong>1,219 kes</strong> (327,253 kes kumulatif);</li>',\n",
       " 'import_cases': '<li>Kes import : <strong>7</strong><strong> </strong><strong>kes </strong>(1 warganegara; 6 bukan warganegara)<strong></strong></li>',\n",
       " 'local_cases': '<li>Kes tempatan : <strong>1,212 kes </strong>[769 warganegara (63.4%); 443 bukan warganegara (36.6%)];</li>',\n",
       " 'active_cases': '<li>Kes aktif :<strong> 15,075 kes;</strong></li>',\n",
       " 'resp_asst_cases': '<li>Kes yang memerlukan rawatan di Unit Rawatan Rapi (ICU) :<strong> 154 kes</strong>;</li>',\n",
       " 'death_cases': '<li>Kes kematian :<strong> 2 kes </strong>(1,220 kes kumulatif, 0.37%; 2 warganegara).</li>',\n",
       " 'number_of_clusters': 'None',\n",
       " 'number_of_new_clusters': 'None',\n",
       " 'number_of_expired_clusters': 'None',\n",
       " 'number_of_active_clusters': 'None',\n",
       " 'date': '17/03/2021'}"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "print(\"hello\")\n",
    "# iterate over Jan 2021 (for now)\n",
    "# start_date = datetime.date(2021, 1, 1)\n",
    "# end_date = datetime.date(2021, 6, 21)\n",
    "# for dt in rrule(DAILY, dtstart=start_date, until=end_date)\n",
    "#     try {\n",
    "#         dataset.append(get_data(dt))\n",
    "#         print(\"Data retrieved for the date {}\".format(dt.strftime(\"%d/%m/%Y\")))\n",
    "#     } catch (e) {\n",
    "#         print(\"Unable to retrieve data for the date {}\".format(dt.strftime(\"%d/%m/%Y\"))\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data retrieved for the date 01/03/2021\n",
      "Data retrieved for the date 02/03/2021\n",
      "Data retrieved for the date 03/03/2021\n",
      "Data retrieved for the date 04/03/2021\n",
      "Data retrieved for the date 05/03/2021\n",
      "Data retrieved for the date 06/03/2021\n",
      "Data retrieved for the date 07/03/2021\n",
      "Data retrieved for the date 08/03/2021\n",
      "Data retrieved for the date 09/03/2021\n",
      "Data retrieved for the date 10/03/2021\n",
      "Data retrieved for the date 11/03/2021\n",
      "Data retrieved for the date 12/03/2021\n",
      "Data retrieved for the date 13/03/2021\n",
      "Data retrieved for the date 14/03/2021\n",
      "Data retrieved for the date 15/03/2021\n",
      "Data retrieved for the date 16/03/2021\n",
      "Data retrieved for the date 17/03/2021\n",
      "Data retrieved for the date 18/03/2021\n",
      "Data retrieved for the date 19/03/2021\n",
      "Data retrieved for the date 20/03/2021\n",
      "Data retrieved for the date 21/03/2021\n",
      "Data retrieved for the date 22/03/2021\n",
      "Data retrieved for the date 23/03/2021\n",
      "Data retrieved for the date 24/03/2021\n",
      "Data retrieved for the date 25/03/2021\n",
      "Data retrieved for the date 26/03/2021\n",
      "Data retrieved for the date 27/03/2021\n",
      "Data retrieved for the date 28/03/2021\n",
      "Data retrieved for the date 29/03/2021\n",
      "Data retrieved for the date 30/03/2021\n",
      "Data retrieved for the date 31/03/2021\n",
      "Data retrieved for the date 01/04/2021\n",
      "Data retrieved for the date 02/04/2021\n",
      "Data retrieved for the date 03/04/2021\n",
      "Data retrieved for the date 04/04/2021\n",
      "Data retrieved for the date 05/04/2021\n",
      "Data retrieved for the date 06/04/2021\n",
      "Data retrieved for the date 07/04/2021\n",
      "Data retrieved for the date 08/04/2021\n",
      "Data retrieved for the date 09/04/2021\n",
      "Data retrieved for the date 10/04/2021\n",
      "Data retrieved for the date 11/04/2021\n",
      "Data retrieved for the date 12/04/2021\n",
      "Data retrieved for the date 13/04/2021\n",
      "Data retrieved for the date 14/04/2021\n",
      "Data retrieved for the date 15/04/2021\n",
      "Data retrieved for the date 16/04/2021\n",
      "Data retrieved for the date 17/04/2021\n",
      "Data retrieved for the date 18/04/2021\n",
      "Data retrieved for the date 19/04/2021\n",
      "Data retrieved for the date 20/04/2021\n",
      "Data retrieved for the date 21/04/2021\n",
      "Data retrieved for the date 22/04/2021\n",
      "Data retrieved for the date 23/04/2021\n",
      "Data retrieved for the date 24/04/2021\n",
      "Data retrieved for the date 25/04/2021\n",
      "Data retrieved for the date 26/04/2021\n",
      "Data retrieved for the date 27/04/2021\n",
      "Data retrieved for the date 28/04/2021\n",
      "Data retrieved for the date 29/04/2021\n",
      "Data retrieved for the date 30/04/2021\n",
      "Data retrieved for the date 01/05/2021\n",
      "Data retrieved for the date 02/05/2021\n",
      "Data retrieved for the date 03/05/2021\n",
      "Data retrieved for the date 04/05/2021\n",
      "Data retrieved for the date 05/05/2021\n",
      "Data retrieved for the date 06/05/2021\n",
      "Data retrieved for the date 07/05/2021\n",
      "Data retrieved for the date 08/05/2021\n",
      "Data retrieved for the date 09/05/2021\n",
      "Data retrieved for the date 10/05/2021\n",
      "Data retrieved for the date 11/05/2021\n",
      "Data retrieved for the date 12/05/2021\n",
      "Data retrieved for the date 13/05/2021\n",
      "Data retrieved for the date 14/05/2021\n",
      "Data retrieved for the date 15/05/2021\n",
      "Data retrieved for the date 16/05/2021\n",
      "Data retrieved for the date 17/05/2021\n",
      "Data retrieved for the date 18/05/2021\n",
      "Data retrieved for the date 19/05/2021\n",
      "Data retrieved for the date 20/05/2021\n",
      "Data retrieved for the date 21/05/2021\n",
      "Data retrieved for the date 22/05/2021\n",
      "Data retrieved for the date 23/05/2021\n",
      "Data retrieved for the date 24/05/2021\n",
      "Data retrieved for the date 25/05/2021\n",
      "Data retrieved for the date 26/05/2021\n",
      "Data retrieved for the date 27/05/2021\n",
      "Data retrieved for the date 28/05/2021\n",
      "Data retrieved for the date 29/05/2021\n",
      "Data retrieved for the date 30/05/2021\n",
      "Data retrieved for the date 31/05/2021\n",
      "Data retrieved for the date 01/06/2021\n",
      "Data retrieved for the date 02/06/2021\n",
      "Data retrieved for the date 03/06/2021\n",
      "Data retrieved for the date 04/06/2021\n",
      "Data retrieved for the date 05/06/2021\n",
      "Data retrieved for the date 06/06/2021\n",
      "Data retrieved for the date 07/06/2021\n",
      "Data retrieved for the date 08/06/2021\n",
      "Data retrieved for the date 09/06/2021\n",
      "Data retrieved for the date 10/06/2021\n",
      "Data retrieved for the date 11/06/2021\n",
      "Data retrieved for the date 12/06/2021\n",
      "Data retrieved for the date 13/06/2021\n",
      "Data retrieved for the date 14/06/2021\n",
      "Data retrieved for the date 15/06/2021\n",
      "Data retrieved for the date 16/06/2021\n",
      "Data retrieved for the date 17/06/2021\n",
      "Data retrieved for the date 18/06/2021\n",
      "Data retrieved for the date 19/06/2021\n",
      "Data retrieved for the date 20/06/2021\n",
      "Data retrieved for the date 21/06/2021\n"
     ]
    }
   ],
   "source": [
    "get_data(datetime.date(2021, 3, 17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert data from march onwards into dataset\n",
    "dataset = []\n",
    "start_date = datetime.date(2021, 3, 1)\n",
    "end_date = datetime.date.today()\n",
    "\n",
    "for dt in rrule(DAILY, dtstart=start_date, until=end_date):\n",
    "    try:\n",
    "        dataset.append(get_data(dt))\n",
    "        print(\"Data retrieved for the date {}\".format(dt.strftime(\"%d/%m/%Y\")))\n",
    "    except:\n",
    "        print(\"Unable to retrieve data for the date {}\".format(dt.strftime(\"%d/%m/%Y\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to csv\n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_csv(r'covid_data.csv')"
   ]
  }
 ]
}